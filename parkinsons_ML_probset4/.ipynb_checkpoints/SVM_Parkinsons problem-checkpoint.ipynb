{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Diagnosing Parkinson’s disease voice signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the medical application of diagnosing Parkinson’s disease from a person’s voice. We consider the data from Little et al. (2009), which can be obtained from the well-known UCI benchmark repository Frank and Asuncion (2010).\n",
    "\n",
    "The data were collected from 31 people, 23 suffering from Parkinson’s disease. Several voice recordings of these people were processed. Each line in the data files corresponds to one recording. The first 22 columns are features derived from the recording, including minimum, average and maximum vocal fundamental frequency, several measures of variation in fundamental frequency, several measures of variation in amplitude, two measures of ratio of noise to tonal components in the voice status, two nonlinear dynamical complexity measures, a measure called signal fractal scaling exponent, as well as nonlinear measures of fundamental frequency variation Little et al. (2009), Frank and Asuncion (2010). The last column is the target label indicating whether the subject is healthy (0) or suffers from Parkinson’s disease (1). The data were split into a training and test set, parkinsonsTrainStatML.dt and parkinsonsTestStatML.dt, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.loadtxt(\"parkinsonsTrainStatML.dt\")\n",
    "data_test = np.loadtxt(\"parkinsonsTestStatML.dt\")\n",
    "\n",
    "train_label = data_train[:,-1]\n",
    "test_label = data_test[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the training data in parkinsonsTrainStatML.dt. Compute the mean and the variance of every input feature (i.e., of every component of the input vector). Find the affine linear mapping fnorm : $R^{22} \\rightarrow R^{22}$ that transforms the training data such that the mean and the variance of every feature in the trans- formed data are 0 and 1, respectively (verify by computing these values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature = data_train[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the mean and variance of all features (22 of them, that means axis 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_train = np.mean(data_feature, axis = 0)\n",
    "sig_train = np.var(data_feature, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the training data using the mean and variance with formula:\n",
    "\n",
    "$$ f_{norm} = \\frac{x-\\mu_{\\text{train data}}}{\\sqrt{var(\\text{train data})}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_data = (data_train[:,:-1]-mu_train)/np.sqrt(sig_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we should now have a mean of zero and a variance of one, lets see what it is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the normalized training data is -2.307e-17 and variance 1.0\n"
     ]
    }
   ],
   "source": [
    "mu_train_new = np.mean(new_training_data)\n",
    "sig_train_new = np.var(new_training_data)\n",
    "\n",
    "print(f\"The mean of the normalized training data is {mu_train_new:.3e} and variance {sig_train_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is not zero due to rounding errors.\n",
    "\n",
    "We also need to normalize the test data, we do this with the same $f_{norm}$ as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the normalized test data is 1.267e-01 and variance 1.5727196891758013\n"
     ]
    }
   ],
   "source": [
    "new_test_data = (data_test[:,:-1]-mu_train)/np.sqrt(sig_train)\n",
    "\n",
    "mu_test_new = np.mean(new_test_data)\n",
    "sig_test_new = np.var(new_test_data)\n",
    "\n",
    "print(f\"The mean of the normalized test data is {mu_test_new:.3e} and variance {sig_test_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection using grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of your SVM classifier depends on the choice of the regularization parameter $C$ and the kernel parameters. \n",
    "We use the radial basis function kernel and will thus vary $\\gamma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = 10**(np.linspace(-2,2,7))  #[0.1, 1, 10, 100, 1000]\n",
    "gamma_list = 10**(np.linspace(-3,1,7))  #[.001, 0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have some validation data to test the cross-validation on. This is chosen randomly from the training data. We take 20 %. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was 0.95 found from [C, gamma] = [1.0, 0.021544346900318832]\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "scores = []\n",
    "for C in C_list:\n",
    "    for gamma in gamma_list:\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(new_training_data[:,:-1], train_label, test_size=0.2, random_state=42)\n",
    "        clf = SVC(C=C, gamma=gamma)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(clf.score(X_valid, y_valid))\n",
    "        \n",
    "        if scores[-1] > score:\n",
    "            score = scores[-1]\n",
    "            best = [C, gamma]\n",
    "            \n",
    "print(f\"The best score was {score} found from [C, gamma] = {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train on the full training data set using the optimal parameters $C$ and $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test score was 0.866.\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=best[0], gamma=best[1])\n",
    "clf.fit(new_training_data[:,:-1], train_label)\n",
    "test_score = clf.score(new_test_data[:,:-1], test_label)\n",
    "\n",
    "print(f\"The test score was {test_score:3.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
